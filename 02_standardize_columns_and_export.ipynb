{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6449b14",
   "metadata": {},
   "source": [
    "# Notebook 02 - Standardize Columns and Export Clean Tables\n",
    "\n",
    "This notebook loads the raw snapshot files, corrects header offsets, and applies consistent column naming across all datasets. The goal is to ensure uniform structure across files for merging, translation, and matching tasks in later notebooks.\n",
    "\n",
    "We rename ambiguous fields (e.g. `Unnamed: 3`) and export each cleaned dataset to CSV format under `cleaned_data/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76e5bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Raw and output paths\n",
    "data_folder = r\"C:\\Users\\User\\Desktop\\University\\BEP\\Data\\Dirk data\"\n",
    "output_folder = \"cleaned_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d854a2d",
   "metadata": {},
   "source": [
    "## Cleaning Strategy\n",
    "\n",
    "We apply the following principles across datasets:\n",
    "\n",
    "- Header offset is handled using `skiprows` (as determined in Notebook 01)\n",
    "- Column names are:\n",
    "  - Lowercased\n",
    "  - Stripped of whitespace\n",
    "  - Converted to snake_case\n",
    "- Known ambiguous or unnamed columns are renamed manually\n",
    "- A `product_name_clean` column is added to the waste dataset for semantic matching\n",
    "\n",
    "We now begin cleaning each file individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7908bdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns:\n",
      "['Store', 'Date', 'Article', 'Unnamed: 3', 'Product name', 'Brand', 'Content', 'Eenheid CE', 'Supplier', 'Unnamed: 9', 'Content category', 'Waste reason', 'Items wasted', 'Value wasted']\n"
     ]
    }
   ],
   "source": [
    "# Load waste data (correct header row)\n",
    "waste_path = os.path.join(data_folder, \"2025-01-24T07_01_23+00_00zero waste lab Mark Down_Waste 2025-01-24.xlsx\")\n",
    "df_waste = pd.read_excel(waste_path, sheet_name=0, skiprows=2)\n",
    "\n",
    "# Preview raw columns\n",
    "print(\"Original columns:\")\n",
    "print(df_waste.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179e62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy with renamed and standardized columns\n",
    "df_waste_cleaned = df_waste.rename(columns={\n",
    "    \"Store\": \"store\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Article\": \"article\",\n",
    "    \"Product name\": \"product_name\",\n",
    "    \"Brand\": \"brand\",\n",
    "    \"Content\": \"content\",\n",
    "    \"Eenheid CE\": \"unit\",\n",
    "    \"Supplier\": \"supplier\",\n",
    "    \"Content category\": \"content_category\",\n",
    "    \"Waste reason\": \"waste_reason\",\n",
    "    \"Items wasted\": \"items_wasted\",\n",
    "    \"Value wasted\": \"value_wasted\"\n",
    "})\n",
    "\n",
    "# Drop irrelevant unnamed columns\n",
    "df_waste_cleaned = df_waste_cleaned.drop(columns=[col for col in df_waste_cleaned.columns if \"Unnamed\" in col])\n",
    "\n",
    "# Clean product name for canonical usage\n",
    "def clean_product_name(name):\n",
    "    if isinstance(name, str):\n",
    "        return name.lower().strip()\n",
    "    return name\n",
    "\n",
    "df_waste_cleaned[\"product_name_clean\"] = df_waste_cleaned[\"product_name\"].apply(clean_product_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07cf9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waste data cleaned and saved to: cleaned_data\\waste_snapshot_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Export cleaned version\n",
    "waste_out_path = os.path.join(output_folder, \"waste_snapshot_cleaned.csv\")\n",
    "df_waste_cleaned.to_csv(waste_out_path, index=False)\n",
    "\n",
    "print(\"Waste data cleaned and saved to:\", waste_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83f7681",
   "metadata": {},
   "source": [
    "## Clean Mark Down Discount File\n",
    "\n",
    "This dataset contains products that were discounted at each store. It includes article identifiers, discount percentage, and regular price per product. We correct for header offset, remove unnamed columns, and standardize naming conventions to align it with the waste dataset.\n",
    "\n",
    "Key columns we aim to standardize:\n",
    "- `Filiaal` -> `store`\n",
    "- `Date`, `Time`, `Article` -> `date`, `time`, `article`\n",
    "- `Discount percentage` -> `discount_percentage`\n",
    "- `Regular price` -> `regular_price`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03d6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load markdown file\n",
    "markdown_path = os.path.join(data_folder, \"2025-01-24T07_03_00+00_00zero waste lab Mark Down 2025-01-24.xlsx\")\n",
    "df_markdown = pd.read_excel(markdown_path, sheet_name=0, skiprows=2)\n",
    "\n",
    "# Rename columns\n",
    "df_markdown_cleaned = df_markdown.rename(columns={\n",
    "    \"Filiaal\": \"store\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Time\": \"time\",\n",
    "    \"Article\": \"article\",\n",
    "    \"Discount percentage\": \"discount_percentage\",\n",
    "    \"Regular price\": \"regular_price\",\n",
    "    \"Pakking price\": \"packing_price\",\n",
    "    \"total amount discounted\": \"discounted_quantity\"\n",
    "})\n",
    "\n",
    "# Drop generic unnamed columns\n",
    "df_markdown_cleaned = df_markdown_cleaned.drop(columns=[col for col in df_markdown_cleaned.columns if \"Unnamed\" in col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17c12e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Markdown data cleaned and saved to: cleaned_data\\markdown_snapshot_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "markdown_out_path = os.path.join(output_folder, \"markdown_snapshot_cleaned.csv\")\n",
    "df_markdown_cleaned.to_csv(markdown_out_path, index=False)\n",
    "\n",
    "print(\" Markdown data cleaned and saved to:\", markdown_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597445a5",
   "metadata": {},
   "source": [
    "## Clean Sales Data File (Part I)\n",
    "\n",
    "This dataset contains transaction-level sales records for a single day. It includes product identifiers, pricing information, promotion status, and quantities sold.\n",
    "\n",
    "We apply the same conventions used for the waste and markdown datasets. Key columns to standardize:\n",
    "\n",
    "- `Store`, `Date`, `Article` -> `store`, `date`, `article`\n",
    "- `Discount 0/1` -> `discount_flag`\n",
    "- `Promotion` -> `promotion`\n",
    "- `Theoretische Kassaverkoopprijs` -> `price_theoretical`\n",
    "- `Selling price` -> `price_sold`\n",
    "- `Items`, `Volume`, `Sold value` -> `items_sold`, `volume_sold`, `revenue_sold`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ec73971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sales data I\n",
    "sales_path = os.path.join(data_folder, \"2025-01-24T07_02_33+00_00zero waste lab_Salesdata I 2025-01-24.xlsx\")\n",
    "df_sales = pd.read_excel(sales_path, sheet_name=0, skiprows=2)\n",
    "\n",
    "# Rename columns\n",
    "df_sales_cleaned = df_sales.rename(columns={\n",
    "    \"Store\": \"store\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Article\": \"article\",\n",
    "    \"product category\": \"product_category\",\n",
    "    \"Discount 0/1\": \"discount_flag\",\n",
    "    \"Promotion\": \"promotion\",\n",
    "    \"Theoretische Kassaverkoopprijs\": \"price_theoretical\",\n",
    "    \"Selling price\": \"price_sold\",\n",
    "    \"items\": \"items_sold\",\n",
    "    \"Volume\": \"volume_sold\",\n",
    "    \"Sold value\": \"revenue_sold\"\n",
    "})\n",
    "\n",
    "# Drop unnamed or placeholder columns\n",
    "df_sales_cleaned = df_sales_cleaned.drop(columns=[col for col in df_sales_cleaned.columns if \"Unnamed\" in col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967ead29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales data cleaned and saved to: cleaned_data\\sales_snapshot_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "sales_out_path = os.path.join(output_folder, \"sales_snapshot_cleaned.csv\")\n",
    "df_sales_cleaned.to_csv(sales_out_path, index=False)\n",
    "\n",
    "print(\"Sales data cleaned and saved to:\", sales_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dda169",
   "metadata": {},
   "source": [
    "## Clean Delivery File (Leveringen Filialen)\n",
    "\n",
    "This file contains delivery volumes for each product per store on a specific day. It includes subgroup codes, article numbers, and the number of units delivered.\n",
    "\n",
    "We apply the following column standardizations:\n",
    "\n",
    "- `Filiaal` -> `store`\n",
    "- `Datum` -> `date`\n",
    "- `Artikel` -> `article`\n",
    "- `Aantal Ontvangen CE` -> `delivered_quantity`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a169929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load delivery data\n",
    "delivery_path = os.path.join(data_folder, \"2025-01-24T06_03_32+00_00leveringen filialen c.e..xlsx\")\n",
    "df_delivery = pd.read_excel(delivery_path, sheet_name=0, skiprows=2)\n",
    "\n",
    "# Rename columns\n",
    "df_delivery_cleaned = df_delivery.rename(columns={\n",
    "    \"Filiaal\": \"store\",\n",
    "    \"Datum\": \"date\",\n",
    "    \"Artikel\": \"article\",\n",
    "    \"Aantal Ontvangen CE\": \"delivered_quantity\"\n",
    "})\n",
    "\n",
    "# Drop columns we donâ€™t use (e.g., 'Subgroep', unnamed)\n",
    "df_delivery_cleaned = df_delivery_cleaned.drop(columns=[col for col in df_delivery_cleaned.columns if \"Unnamed\" in col or \"Subgroep\" in col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fffd242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delivery data cleaned and saved to: cleaned_data\\delivery_snapshot_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "delivery_out_path = os.path.join(output_folder, \"delivery_snapshot_cleaned.csv\")\n",
    "df_delivery_cleaned.to_csv(delivery_out_path, index=False)\n",
    "\n",
    "print(\"Delivery data cleaned and saved to:\", delivery_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c28573",
   "metadata": {},
   "source": [
    "## Clean Store Metadata (NAW Filialen)\n",
    "\n",
    "This file contains branch-level metadata, including the name, address, and postal code of each store location.\n",
    "\n",
    "We standardize the following columns:\n",
    "\n",
    "- `Fil. Nr.` -> `store`\n",
    "- `Filiaal` -> `store_name`\n",
    "- `Adres` -> `address`\n",
    "- `Postcode` -> `postal_code`\n",
    "- `Plaatsnaam` -> `city`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05edda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NAW filialen\n",
    "naw_path = os.path.join(data_folder, \"NAW filialen.xlsx\")\n",
    "df_naw = pd.read_excel(naw_path, sheet_name=0)\n",
    "\n",
    "# Rename columns\n",
    "df_naw_cleaned = df_naw.rename(columns={\n",
    "    \"Fil. Nr.\": \"store\",\n",
    "    \"Filiaal\": \"store_name\",\n",
    "    \"Adres\": \"address\",\n",
    "    \"Postcode\": \"postal_code\",\n",
    "    \"Plaatsnaam\": \"city\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c64e233b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store metadata cleaned and saved to: cleaned_data\\store_metadata_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "naw_out_path = os.path.join(output_folder, \"store_metadata_cleaned.csv\")\n",
    "df_naw_cleaned.to_csv(naw_out_path, index=False)\n",
    "\n",
    "print(\"Store metadata cleaned and saved to:\", naw_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69243ccd",
   "metadata": {},
   "source": [
    "## Summary and Outputs\n",
    "\n",
    "This notebook processed and standardized the following files:\n",
    "\n",
    "- Waste data -> `waste_snapshot_cleaned.csv`\n",
    "- Markdown (discount) data -> `markdown_snapshot_cleaned.csv`\n",
    "- Sales data I -> `sales_snapshot_cleaned.csv`\n",
    "- Delivery data -> `delivery_snapshot_cleaned.csv`\n",
    "- Store metadata -> `store_metadata_cleaned.csv`\n",
    "\n",
    "All files have been cleaned for:\n",
    "- Header offsets\n",
    "- Inconsistent or missing column names\n",
    "- Irregular capitalization and formatting\n",
    "\n",
    "Cleaned outputs are saved under the `cleaned_data/` folder and will serve as the foundation for merging, translation, semantic matching, and evaluation workflows in subsequent notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce73516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
