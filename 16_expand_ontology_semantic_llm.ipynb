{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03810321",
   "metadata": {},
   "source": [
    "# Notebook 16 - Expand Ontology with Semantic and LLM-based Matching\n",
    "\n",
    "## Purpose\n",
    "This notebook improves the concept ontology used for matching products to recipes by resolving inconsistencies, rare expressions, and ambiguous terms. It employs two complementary techniques:\n",
    "\n",
    "- **Semantic Embedding Matching**: Leverages multilingual Sentence-BERT to detect concept-level similarities between terms, accounting for linguistic variation and phrasing.\n",
    "- **LLM-based Expansion**: Uses GPT (or simulated alternatives) to suggest generalized concepts for noisy, branded, or compound entries.\n",
    "\n",
    "These methods increase ontology coverage, improve robustness to multilingual and brand-specific variation, and support more accurate and scalable matching pipelines.\n",
    "\n",
    "## Inputs\n",
    "- `ontology_manual.csv`: Initial raw_term -> concept mappings, manually curated.\n",
    "- `recipes_with_ontology.csv`: Recipe ingredients tagged with potentially incomplete or inconsistent concepts.\n",
    "- `products_with_priority.csv`: Products tagged with product concepts and priority metadata.\n",
    "- *(Optional)* OpenAI API key for GPT-powered expansion.\n",
    "\n",
    "## Outputs\n",
    "- `ontology_expanded.csv`: Final enriched ontology with updated concept assignments.\n",
    "- `gpt_candidates.csv`: Table of flagged low-frequency or misaligned terms considered for concept normalization.\n",
    "- Printed suggestions from Sentence-BERT and GPT or manual substitutes for audit and refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "62a0409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Optional GPT Expansion\n",
    "try:\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "except ImportError:\n",
    "    openai = None\n",
    "\n",
    "# Define folders and files\n",
    "input_folder = \"cleaned_data\"\n",
    "output_folder = \"ontology_exports\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "ontology_file = os.path.join(input_folder, \"ontology_manual.csv\")\n",
    "recipes_file = os.path.join(input_folder, \"recipes_with_ontology.csv\")\n",
    "products_file = os.path.join(input_folder, \"products_with_priority.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c987375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  Ontology: (6, 2)\n",
      "  Recipes: (6, 6)\n",
      "  Products: (126919, 35)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (1,3,5,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_ontology = pd.read_csv(ontology_file)\n",
    "df_recipes = pd.read_csv(recipes_file)\n",
    "df_products = pd.read_csv(products_file)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(f\"  Ontology: {df_ontology.shape}\")\n",
    "print(f\"  Recipes: {df_recipes.shape}\")\n",
    "print(f\"  Products: {df_products.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7fb74f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376ceac4964d488f962abcde6da663a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic suggestions generated: 0\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v2\")\n",
    "\n",
    "raw_terms = df_ontology[\"raw_term\"].astype(str).tolist()\n",
    "embeddings = model.encode(raw_terms, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "cosine_sim = util.pytorch_cos_sim(embeddings, embeddings).cpu().numpy()\n",
    "np.fill_diagonal(cosine_sim, 0)\n",
    "\n",
    "threshold = 0.85\n",
    "suggestions = []\n",
    "for i, term in enumerate(raw_terms):\n",
    "    j = np.argmax(cosine_sim[i])\n",
    "    score = cosine_sim[i][j]\n",
    "    if score >= threshold:\n",
    "        suggestions.append((term, raw_terms[j], score))\n",
    "\n",
    "df_semantic = pd.DataFrame(suggestions, columns=[\"term\", \"suggested_concept\", \"similarity\"])\n",
    "df_semantic = df_semantic.drop_duplicates(\"term\")\n",
    "print(\"Semantic suggestions generated:\", len(df_semantic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cfd0cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 rare or inconsistent terms flagged for review.\n"
     ]
    }
   ],
   "source": [
    "df_mismatch = df_ontology[df_ontology[\"raw_term\"].str.lower() != df_ontology[\"concept\"].str.lower()]\n",
    "term_counts = pd.concat([\n",
    "    df_recipes[\"ingredient_concept\"],\n",
    "    df_products[\"product_concept\"]\n",
    "]).dropna().value_counts()\n",
    "\n",
    "rare_terms = term_counts[term_counts <= 2].index.tolist()\n",
    "candidates = set(df_ontology[\"raw_term\"]).intersection(rare_terms)\n",
    "df_candidates = df_ontology[df_ontology[\"raw_term\"].isin(candidates)].copy()\n",
    "\n",
    "print(f\"{len(df_candidates)} rare or inconsistent terms flagged for review.\")\n",
    "df_candidates.to_csv(os.path.join(output_folder, \"gpt_candidates.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1001ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual LLM-style suggestions:\n",
      "       raw_term suggested_concept\n",
      "0  strawberries        strawberry\n",
      "3         honey             honey\n",
      "4        tomato            tomato\n",
      "5          tuna              tuna\n"
     ]
    }
   ],
   "source": [
    "# Simulated normalization mappings\n",
    "manual_concepts = {\n",
    "    \"strawberries\": \"strawberry\",\n",
    "    \"honey\": \"honey\",\n",
    "    \"tomato\": \"tomato\",\n",
    "    \"tuna\": \"tuna\"\n",
    "}\n",
    "\n",
    "df_candidates[\"suggested_concept\"] = df_candidates[\"raw_term\"].map(manual_concepts)\n",
    "print(\"Manual LLM-style suggestions:\")\n",
    "print(df_candidates[[\"raw_term\", \"suggested_concept\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9e1a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge updated mappings into the original ontology\n",
    "update_map = dict(zip(df_candidates[\"raw_term\"], df_candidates[\"suggested_concept\"]))\n",
    "df_ontology[\"concept\"] = df_ontology.apply(\n",
    "    lambda row: update_map[row[\"raw_term\"]] if row[\"raw_term\"] in update_map else row[\"concept\"],\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "233485c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final ontology exported to: ontology_exports\\ontology_expanded.csv\n"
     ]
    }
   ],
   "source": [
    "expanded_file = os.path.join(output_folder, \"ontology_expanded.csv\")\n",
    "df_ontology.to_csv(expanded_file, index=False)\n",
    "print(f\"Final ontology exported to: {expanded_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
